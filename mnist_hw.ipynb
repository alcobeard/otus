{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist hw.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOGRruXRV2JB/qsR2CT5hIr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alcobeard/otus/blob/master/mnist_hw.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-00zzG0oUEw-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp \"/content/drive/My Drive/Colab Notebooks/DL Otus/Homework/2/utils.py\" ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSTwsVA5WFoL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/DL Otus/Homework/2/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzfy0JJX8bAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import subprocess\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from utils import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvBavNue82sZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Если доступна видеокрта - будем считаться на ней\n",
        "# Если нет - на ЦПУ\n",
        "# Обязательно используем .to(device) на torch.tensor()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda:0')\n",
        "    print(subprocess.getoutput(\"nvidia-smi\"))\n",
        "    print()\n",
        "    print(f\"GPUs in  system:        {torch.cuda.device_count()}\")\n",
        "    print(f\"GPU compute capability: {torch.cuda.get_device_capability(device)}\")\n",
        "    print(f\"GPU name:               {torch.cuda.get_device_name()}\")\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "device"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqXtIlOxQV8V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYPcvlPOQrCg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peeMpZgt82vf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Описываем класс, создающий сеть\n",
        "class Net(nn.Module):\n",
        "    # по умолчанию log_softmax = False\n",
        "    def __init__(self, log_softmax=False):\n",
        "        super(Net, self).__init__()\n",
        "        inputs=28*28\n",
        "        l1_hidden_neurons=128\n",
        "        outputs=10\n",
        "        self.fc1 = nn.Linear(inputs, l1_hidden_neurons)\n",
        "        self.act1 = torch.nn.Sigmoid() # именно эта строка не используется в forward, дописал для понимания последовательности действий\n",
        "        self.fc2 = nn.Linear(l1_hidden_neurons, outputs)\n",
        "        self.log_softmax = log_softmax\n",
        "        self.optim = optim.SGD(self.parameters(), lr=1.0)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # на вход поступает набор картинок, размерность тензора (N, 28, 28)\n",
        "        # нужно \"выпрямить\" в размерность (N, 28*28)\n",
        "        x = x.view(-1, 28*28)\n",
        "        \n",
        "        # тут два шага в одном - суммируем данные первым слоем и сразу же пропускаем через активацию\n",
        "        x = torch.sigmoid(self.fc1(x))\n",
        "        \n",
        "        # суммируем выходы первого слоя\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        # и в зависимости от запрошенного, используем разные softmax'ы\n",
        "        if self.log_softmax:\n",
        "            # если указали log_softmax=True\n",
        "            x = F.log_softmax(x, dim=1) # log_softmax\n",
        "        else:\n",
        "            # по умолчанию\n",
        "            x = torch.log(F.softmax(x, dim=1)) # log от softmax\n",
        "        return x\n",
        "    \n",
        "    def loss(self, output, target, **kwargs):\n",
        "        self._loss = F.nll_loss(output, target, **kwargs)\n",
        "        return self._loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fczbQ3282ys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(models, epoch, train_stats):\n",
        "    # читаем батчами по 50 экземпляров каждый (настройки dataset_loader)\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # данные перемещаем на нужное устройство (CPU/GPU)\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        \n",
        "        # на одном и том же батче обучаем каждую модель\n",
        "        # это экономит время на передачу данных в\\из памяти\n",
        "        for model in models:\n",
        "            # обнулили накопленные градиенты\n",
        "            model.optim.zero_grad()\n",
        "            # посчитали ответы сети с текущими весами\n",
        "            output = model.forward(data)\n",
        "            # посчитали ошибку\n",
        "            loss = model.loss(output, target)\n",
        "            # посчитали градиент\n",
        "            loss.backward()\n",
        "            # и проапдейтили веса новыми значениями \n",
        "            model.optim.step()\n",
        "        \n",
        "        # раз в (batch_size * 200) == 10000 выведем статистику  \n",
        "        if batch_idx % 200 == 0:\n",
        "            epoch_stats = f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({round(100. * batch_idx / len(train_loader), 2)}%)]\\t\\t\"\n",
        "            loss_stats = f\"Losses: \"\n",
        "            for idx, model in enumerate(models):\n",
        "                train_stats[idx] += [model._loss.item()]\n",
        "                loss_stats += f\" {idx}: {round(model._loss.item(),4)}\\t\"\n",
        "            print(epoch_stats + loss_stats)\n",
        "            \n",
        "    # перед завершением функции покажем посленюю статистику, т.к. она не попала  в цикл        \n",
        "    batch_idx += 1 # потому что индекс с нуля\n",
        "    epoch_stats = f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({round(100. * batch_idx / len(train_loader), 2)}%)]\\t\\t\"\n",
        "    loss_stats = f\"Losses: \"\n",
        "    for idx, model in enumerate(models):\n",
        "        train_stats[idx] += [model._loss.item()]\n",
        "        loss_stats += f\" {idx}: {round(model._loss.item(),4)}\\t\"\n",
        "    print(epoch_stats + loss_stats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKDvaS8_8206",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(models, test_stats):\n",
        "    # будем считать статистики по каждой модели отдельно\n",
        "    test_loss = [0]*len(models) # сюда накапливать величину лосса\n",
        "    correct = [0]*len(models) # а сюда плюсовать правильные ответы\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            # тестовые данные закидываем на CPU\\GPU:\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "            \n",
        "            # смотрим какие ответы нам дает каждая модель на тестовой выборке:\n",
        "            output = []\n",
        "            for model in models:\n",
        "                output += [model.forward(data)]\n",
        "\n",
        "            # теперь подсчтаем статистики для каждой модели\n",
        "            for i, model in enumerate(models):\n",
        "                # запишем сумму ошибок каждой модели\n",
        "                test_loss[i] += model.loss(output[i], target, reduction='sum').item()\n",
        "                # запишем индекс самого вероятного класса каждой модели\n",
        "                pred = output[i].data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "                # плюсик в правильные ответы модели если ответ совпал с правильным\n",
        "                correct[i] += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "    # посчитали сумму лосса и количество правильных ответов каждой модели\n",
        "    for i in range(len(models)):\n",
        "        # теперь усредним лосс каждой модели\n",
        "        test_loss[i] /= len(test_loader.dataset)\n",
        "    # сумму правильных ответов каждой модели делим на воличество примеров, получаем процент правильных ответов каждой модели в массив\n",
        "    correct_pct = [100. * c / len(test_loader.dataset) for c in correct]\n",
        "    # и вывод результата подсчета\n",
        "    # 0: Loss: 0.1010\tAccuracy: 9679/10000 (97%)\n",
        "    lines =\"\"\n",
        "    for i, model in enumerate(models):\n",
        "        test_stats[i] += [test_loss[i]]\n",
        "        lines += f\"Model #{i}\\t Loss: {round(test_loss[i],4)}\\t \"\n",
        "        lines += f\"Accuracy:{correct[i]}/{len(test_loader.dataset)} \"\n",
        "        lines += f\"({round(correct_pct[i].item(),2)}%)\\n\"\n",
        "    report = 'Test set:\\n' + lines\n",
        "    \n",
        "    print(report)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBeyeEcM9QBO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Загрузим датасет MNIST, процедура загрузки описана в допфайле utils.py\n",
        "train_loader, test_loader = mnist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxh4nLAm9QKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# создадим две модели, первая с log(softmax), вторая с log_softmax\n",
        "models = [Net().to(device), Net(True).to(device)]\n",
        "\n",
        "epoch_count = 2\n",
        "\n",
        "# будем накапливать статистику для построения графиков\n",
        "train_stats = []\n",
        "test_stats = []\n",
        "# зададим массив так, чтобы первым элементом накапливался массив по первой модели, вторым по второй и т.д.\n",
        "for i, model in enumerate(models):\n",
        "    train_stats += [[]]\n",
        "    test_stats += [[]]\n",
        "\n",
        "# стартуем обучение сетей\n",
        "for epoch in range(1, epoch_count+1):\n",
        "    train(models, epoch, train_stats)\n",
        "    test(models,test_stats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVKfSFEj9QNO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, model in enumerate(models):\n",
        "    train_stats[i].pop(0)\n",
        "    normalized_test_stats = []\n",
        "    for j in test_stats[i]:\n",
        "        normalized_test_stats.extend([j]*7)\n",
        "    normalized_test_stats.pop(0)\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.title(f\"Model #{i}\")\n",
        "    plt.plot(train_stats[i], linestyle=\"-\", color=[0.1, .1, .1, .5], label=\"train\")\n",
        "    plt.plot(normalized_test_stats, linestyle=\"-\", color=[1, .1, .1, 1.0], label=\"test\")\n",
        "    plt.legend(loc=\"upper right\")\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0eR609y_VXW",
        "colab_type": "text"
      },
      "source": [
        "### Переобучение модели\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcFuRGde_Zvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Описываем класс, создающий сеть\n",
        "class Net(nn.Module):\n",
        "    # по умолчанию log_softmax = False\n",
        "    def __init__(self, log_softmax=False):\n",
        "        super(Net, self).__init__()\n",
        "        inputs=28*28\n",
        "        l1_hidden_neurons=256\n",
        "        outputs=10\n",
        "        self.fc1 = nn.Linear(inputs, l1_hidden_neurons)\n",
        "        self.act1 = torch.nn.Sigmoid() # именно эта строка не используется в forward, дописал для понимания последовательности действий\n",
        "        self.fc2 = nn.Linear(l1_hidden_neurons, outputs)\n",
        "        self.log_softmax = log_softmax\n",
        "        self.optim = optim.SGD(self.parameters(), lr=0.1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # на вход поступает набор картинок, размерность тензора (N, 28, 28)\n",
        "        # нужно \"выпрямить\" в размерность (N, 28*28)\n",
        "        x = x.view(-1, 28*28)\n",
        "        \n",
        "        # тут два шага в одном - суммируем данные первым слоем и сразу же пропускаем через активацию\n",
        "        x = torch.sigmoid(self.fc1(x))\n",
        "        \n",
        "        # суммируем выходы первого слоя\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        # и в зависимости от запрошенного, используем разные softmax'ы\n",
        "        if self.log_softmax:\n",
        "            # если указали log_softmax=True\n",
        "            x = F.log_softmax(x, dim=1) # log_softmax\n",
        "        else:\n",
        "            # по умолчанию\n",
        "            x = torch.log(F.softmax(x, dim=1)) # log от softmax\n",
        "        return x\n",
        "    \n",
        "    def loss(self, output, target, **kwargs):\n",
        "        self._loss = F.nll_loss(output, target, **kwargs)\n",
        "        return self._loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grxOJCIP_wT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# Достаточно одной модели с log_softmax\n",
        "models = [Net(True).to(device)]\n",
        "\n",
        "epoch_count = 100\n",
        "\n",
        "# будем накапливать статистику для построения графиков\n",
        "train_stats = []\n",
        "test_stats = []\n",
        "# зададим массив так, чтобы первым элементом накапливался массив по первой модели, вторым по второй и т.д.\n",
        "for i, model in enumerate(models):\n",
        "    train_stats += [[]]\n",
        "    test_stats += [[]]\n",
        "\n",
        "# стартуем обучение сетей\n",
        "for epoch in range(1, epoch_count+1):\n",
        "    train(models, epoch, train_stats)\n",
        "    test(models,test_stats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUdm3-AhDJnh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, model in enumerate(models):\n",
        "    train_stats[i].pop(0)\n",
        "    normalized_test_stats = []\n",
        "    for j in test_stats[i]:\n",
        "        normalized_test_stats.extend([j]*7)\n",
        "    normalized_test_stats.pop(0)\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.title(f\"Model #{i}\")\n",
        "    plt.plot(train_stats[i], linestyle=\"-\", color=[0.1, .1, .1, .5], label=\"train\")\n",
        "    plt.plot(normalized_test_stats, linestyle=\"-\", color=[1, .1, .1, 1.0], label=\"test\")\n",
        "    plt.legend(loc=\"upper right\")\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}