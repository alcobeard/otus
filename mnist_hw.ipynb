{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist hw.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "mount_file_id": "1xQxsnMyrtb4HAUUMAglG3czq1hjHZL1c",
      "authorship_tag": "ABX9TyMRjh9W5QJEe1fgaguEbd1Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alcobeard/otus/blob/master/mnist_hw.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkK1aOBY4yOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install jupyterthemes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duCdj7YC4Y0d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from jupyterthemes import jtplot\n",
        "\n",
        "jtplot.style(theme='monokai', context='notebook', ticks=True, grid=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-00zzG0oUEw-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !cp \"/content/drive/My Drive/Colab Notebooks/DL Otus/Homework/2/utils.py\" ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSTwsVA5WFoL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/DL Otus/Homework/2/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzfy0JJX8bAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import subprocess\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from utils import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvBavNue82sZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Если доступна видеокрта - будем считаться на ней\n",
        "# Если нет - на ЦПУ\n",
        "# Обязательно используем .to(device) на torch.tensor()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda:0')\n",
        "    print(subprocess.getoutput(\"nvidia-smi\"))\n",
        "    print()\n",
        "    print(f\"GPUs in  system:        {torch.cuda.device_count()}\")\n",
        "    print(f\"GPU compute capability: {torch.cuda.get_device_capability(device)}\")\n",
        "    print(f\"GPU name:               {torch.cuda.get_device_name()}\")\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "device"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYPcvlPOQrCg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcFuRGde_Zvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Описываем класс, создающий сеть\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, log_softmax=False):\n",
        "        super(Net, self).__init__()\n",
        "        inputs=28*28\n",
        "        l1_hidden_neurons=5\n",
        "        outputs=10\n",
        "        self.fc1 = nn.Linear(inputs, l1_hidden_neurons)\n",
        "        self.act1 = torch.nn.Sigmoid() # не используется в forward\n",
        "        self.fc2 = nn.Linear(l1_hidden_neurons, 128)\n",
        "        self.fc3 = nn.Linear(128, outputs)\n",
        "        self.log_softmax = log_softmax\n",
        "        self.optim = optim.SGD(self.parameters(), lr=0.1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28) # выпрямляем в вектор размерности 1 на 28х28\n",
        "        \n",
        "        x = torch.sigmoid(self.fc1(x)) # активируем функцию через сигмоид(суммируем данные первым слоем)\n",
        "        x = torch.sigmoid(self.fc2(x)) # активируем функцию через сигмоид(суммируем данные вторым слоем)\n",
        "        \n",
        "        x = self.fc3(x) # суммируем выходы второго слоя\n",
        "        \n",
        "        # и в зависимости от запрошенного, используем разные softmax'ы\n",
        "        if self.log_softmax:\n",
        "            x = F.log_softmax(x, dim=1) # log_softmax\n",
        "        else:\n",
        "            x = torch.log(F.softmax(x, dim=1)) # log от softmax\n",
        "        return x\n",
        "    \n",
        "    def loss(self, output, target, **kwargs):\n",
        "        self._loss = F.nll_loss(output, target, **kwargs)\n",
        "        return self._loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fczbQ3282ys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(models, epoch, train_stats):\n",
        "    # читаем батчами по 50 экземпляров каждый (настройки dataset_loader)\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # данные перемещаем на нужное устройство (CPU/GPU)\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        \n",
        "        for model in models:\n",
        "            # обнулили накопленные градиенты\n",
        "            model.optim.zero_grad()\n",
        "            # посчитали ответы сети с текущими весами\n",
        "            output = model.forward(data)\n",
        "            # посчитали ошибку\n",
        "            loss = model.loss(output, target)\n",
        "            # посчитали градиент\n",
        "            loss.backward()\n",
        "            # и проапдейтили веса новыми значениями \n",
        "            model.optim.step()\n",
        "        \n",
        "        # раз в (batch_size * 200) == 10000 выведем статистику  \n",
        "        if batch_idx % 200 == 0:\n",
        "            epoch_stats = f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({round(100. * batch_idx / len(train_loader), 2)}%)]\\t\\t\"\n",
        "            loss_stats = f\"Losses: \"\n",
        "            for idx, model in enumerate(models):\n",
        "                train_stats[idx] += [model._loss.item()]\n",
        "                loss_stats += f\" {idx}: {round(model._loss.item(),4)}\\t\"\n",
        "            print(epoch_stats + loss_stats)\n",
        "            \n",
        "    # перед завершением функции покажем посленюю статистику, т.к. она не попала  в цикл        \n",
        "    batch_idx += 1 # потому что индекс с нуля\n",
        "    epoch_stats = f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({round(100. * batch_idx / len(train_loader), 2)}%)]\\t\\t\"\n",
        "    loss_stats = f\"Losses: \"\n",
        "    for idx, model in enumerate(models):\n",
        "        train_stats[idx] += [model._loss.item()]\n",
        "        loss_stats += f\" {idx}: {round(model._loss.item(),4)}\\t\"\n",
        "    print(epoch_stats + loss_stats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKDvaS8_8206",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(models, model_test_loss, model_test_accuracy):\n",
        "    # будем считать статистики по каждой модели отдельно\n",
        "    test_loss = [0]*len(models) # сюда накапливать величину лосса\n",
        "    correct = [0]*len(models) # а сюда плюсовать правильные ответы\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            # тестовые данные закидываем на CPU\\GPU:\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "            \n",
        "            # смотрим какие ответы нам дает каждая модель на тестовой выборке:\n",
        "            output = []\n",
        "            for model in models:\n",
        "                output += [model.forward(data)]\n",
        "\n",
        "            # теперь подсчтаем статистики для каждой модели\n",
        "            for i, model in enumerate(models):\n",
        "                # запишем сумму ошибок каждой модели\n",
        "                test_loss[i] += model.loss(output[i], target, reduction='sum').item()\n",
        "                # запишем индекс самого вероятного класса каждой модели\n",
        "                pred = output[i].data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "                # плюсик в правильные ответы модели если ответ совпал с правильным\n",
        "                correct[i] += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "    # посчитали сумму лосса и количество правильных ответов каждой модели\n",
        "    for i in range(len(models)):\n",
        "        # теперь усредним лосс каждой модели\n",
        "        test_loss[i] /= len(test_loader.dataset)\n",
        "    # сумму правильных ответов каждой модели делим на воличество примеров, получаем процент правильных ответов каждой модели в массив\n",
        "    correct_pct = [100. * c / len(test_loader.dataset) for c in correct]\n",
        "    # и вывод результата подсчета\n",
        "    # 0: Loss: 0.1010\tAccuracy: 9679/10000 (97%)\n",
        "    lines =\"\"\n",
        "    for i, model in enumerate(models):\n",
        "        model_test_loss[i] += [test_loss[i]]\n",
        "        model_test_accuracy[i] += [correct[i].item()/len(test_loader.dataset)]\n",
        "        lines += f\"Model #{i}\\t Loss: {round(test_loss[i],4)}\\t \"\n",
        "        lines += f\"Accuracy:{correct[i]}/{len(test_loader.dataset)} \"\n",
        "        lines += f\"({round(correct_pct[i].item(),2)}%)\\n\"\n",
        "    report = 'Test set:\\n' + lines\n",
        "    \n",
        "    print(report)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhFqBJfHYRBG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loader, train_loader = mnist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grxOJCIP_wT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# Достаточно одной модели с log_softmax\n",
        "models = [Net(True).to(device)]\n",
        "\n",
        "epoch_count = 500\n",
        "\n",
        "# будем накапливать статистику для построения графиков\n",
        "model_train_loss = []\n",
        "model_test_loss = []\n",
        "model_test_accuracy = []\n",
        "# зададим массив так, чтобы первым элементом накапливался массив по первой модели, вторым по второй и т.д.\n",
        "for i, model in enumerate(models):\n",
        "    model_train_loss += [[]]\n",
        "    model_test_loss += [[]]\n",
        "    model_test_accuracy += [[]]\n",
        "\n",
        "# стартуем обучение сетей\n",
        "for epoch in range(1, epoch_count+1):\n",
        "    train(models, epoch, model_train_loss)\n",
        "    test(models, model_test_loss, model_test_accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MB2y9M3IfyO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, model in enumerate(models):\n",
        "    model_train_loss[i].pop(0)\n",
        "    normalized_test_loss = []\n",
        "    normalized_test_accuracy = []\n",
        "    for j in model_test_accuracy[i]:\n",
        "        normalized_test_accuracy.extend([j]*2)\n",
        "    for j in model_test_loss[i]:\n",
        "        normalized_test_loss.extend([j]*2)\n",
        "    normalized_test_accuracy.pop(0)\n",
        "    normalized_test_loss.pop(0)\n",
        "\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.title(f\"Model #{i} loss\")\n",
        "    plt.plot(model_train_loss[i], linestyle=\"-\", color=[.153,.255,.255,1], label=\"train\")\n",
        "    plt.plot(normalized_test_loss, linestyle=\"-\", color=[1, .1, .1, 1.0], label=\"test\")\n",
        "    plt.plot(normalized_test_accuracy, linestyle=\"-\", color=[0.1, 1., .1, 0.5], label=\"accuracy\")\n",
        "    plt.legend(loc=\"upper right\")\n",
        "    plt.grid()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}